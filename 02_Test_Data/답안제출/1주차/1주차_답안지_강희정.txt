1,2-(1) O
1,2-(2) 특성공학
1,2-(3) 옵티마이저
1,2-(4) 한 행렬의 행의 크기와 다른 행렬의 열의 크기가 같을 때
1,2-(5) SGD  최적화 과정에서 지역 최솟값에 빠지는 문제와 수렴 속도 문제를 모두 해결할 수 있기 때문
3,4-(1) 네트워크가 손실을 최소화하기 위해 편법을 사용할 수 있기 때문, 만약에 목적 함수가 주어진 문제의 성공과 전혀 관련이 없다면 원하지 않는 일을 수행하는 모델이 만들어질 것이다.
3,4-(2) 3
3,4-(3) 5
3,4-(4) 레이어를 자동으로 생성
3,4-(5) X
5,6-(1) 컨브넷이 이미지 오른쪽 아래 모서리에서 어떤 패턴을 학습했다면 다른 곳에서도 이 패턴을 인식할 수 있는 것
5,6-(2) 합성곱 기반 층에 있는 하위 층들은 좀 더 일반적이고 재사용 가능한 특성들을 인코딩하는 반면 상위 층은 좀 더 특화된 특성을 인코딩 따라서 하위 층으로 갈수록 미세 조정에 대한 효과가 감소
5,6-(3) 사용할 텍스트의 길이가 30이기 때문에 텍스트1과 텍스트2를 임베딩하면 텍스트1,2의 길이는 모두 30이 될 것이다.
5,6-(4) 타임스텝마다 랜덤한 드롭아웃 마스크를 적용하면 오차 신호가 전파되는 것을 방해하고 학습 과정에 해를 끼쳐 LOSS 값이 더 낮아 진다.
5,6-(5) 입력과 출력 사이의 거리가 멀어질 수록 연관관계가 적어지는 문제인 장기의존성 문제
7. 8-(1) 질문-응답 모델은 2개의 입력을 가지고, 모델을 출력한다. 가장 간단한 구조는 미리 정의한 어휘 사전에서 소프트 맥스 함수를 통해 한 단어로 된 답을 출력하는 것이다.
7. 8-(2)    ① keras.callbacks.EarlyStopping - 과대적합이 일어나기 전에 일찍 훈련을 멈춤
    ② keras.callbacks.LearningRateScheduler - 에포크에 따라 학습률을 조정
7. 8-(3) 탐욕적 샘플링 - 반복적이고 예상 가능한 문자열을 만드는 방법
          확률적 샘플링 - 글자의 확률 분포에서 샘플링하는 과정에 무작위성을 주입하는 방법
          소프트맥스 역할 - 샘플링 과정에서 확률적인 양을 조절하기위해 사용
7. 8-(4) 옥타브
7. 8-(5) Z 포인트가 남자 얼굴의 임베딩된 표현이기 때문에 잠재공간의 Z + S 포인트는 남자 얼굴이 웃고있는 이미지가 될 것이다.
7. 8-(6) 생성자-생성자를 받아 실제 데이터와 비슷한 데이터를 만들어내도록 학습
          구분자- 실제 데이터와 생성자가 생성한 가짜 데이터를 구별하도록 학습
1,2-(1) 0
1,2-(2) 데이터 전처리
1,2-(3) 옵티마이저
1,2-(4) assert x.shape[0] == y.shape[0]
1,2-(5)
3,4-(1) 우리가 만든 모든 신경망은 단지 손실 함수를 최소화하기만 하기 때문에
3,4-(2) 3번
3,4-(3) 5
3,4-(4) 레이블
3,4-(5) X
5,6-(1) 이미지의 어떤 패턴을 학습했다면 이미지의 다른 위치에서도 패턴 인식 가능한 것
5,6-(2) 상위 층은 구체적인 특성을 나타냄. 새로운 문제에 재활용할 수 있도록 수정이 필요.
5,6-(3) 
5,6-(4) 간단한 모델을 사용했을 때?
5,6-(5) 그래디언트 소실 문제
7,8-(1) 2개의 입력 >답 출력 필요 > 벡터로 연결, 소프트맥스 함수로 미리 정의한 어휘 사전에서 한 단어로 된 답 출력
7,8-(2) 1. 검증 손실이 향상되지 않을 경우, 훈련을 중지시킨다. 
         2. 에포크에 따라 학습률을 조정한다.
7,8-(3) 1. 탐욕적 샘플링 - 항상 가장 높은 확률을 가진 글자 선택, 반복적이고 예상 가능한 문자열을 만듬 > 논리적인 언어처럼 보이지 않음
         2. 확률적 샘플링- 다음 글자의 확률 분포에서 샘플링하는 과정에 무작위성 주입, 모델의 소프트맥스 출력에 사용하기 좋음, 샘플링 과정에서 무작위성의 양을 조절할 방법이 없음
         3. 소프트맥스 온도- 샘플링 과정에서 확률의 양을 조절하기 위한 파라미터로 얼마나 놀라운 또는 예상되는 글자를 선택할지 결정하는 역할을 함
7,8-(4) 딥드림과정
7,8-(5) 웃고 있는 남자의 얼굴
7,8-(6) 생성자 > 랜덤한 노이즈를 입력받아 이미지 생성 후 손실을 최소화 하는 방법 
         판별자 > 실제 이미지로 훈련한 후 생성자가 생성한 이미지로 훈련

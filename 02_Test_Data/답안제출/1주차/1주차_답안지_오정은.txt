1. 2-(1) (O)
1. 2-(2) 특성공학
1. 2-(3) 옵티마이저
1. 2-(4) x와 y의 크기가 같을때
1. 2-(5) 최적화 과정에서 지역 최솟값에 빠지는 문제점 해결하기 위해

3. 4-(1) 네트워크가 손실을 최소화하기 위해 편법을 사용할 수 있기 때문
만약 목적 함수가 주어진 문제의 성공과 전혀 관련이 없다면 원하지 않는 일을 수행하는 모델이 만들어짐
3. 4-(2) 3
3. 4-(3) 5
3. 4-(4) Label
3. 4-(5) X

5. 6-(1) 컨브넷이 이미지 오른쪽 아래 모서리에서 어떤 패턴을 학습했다면 다른 곳에서도 이 패턴을 인식할 수 있는 것
5. 6-(2) 합성곱 기반 층에 있는 하위 층들은 좀 더 일반적이고 재사용 가능한 특성들을 인코딩하는 반면 상위 층은 좀 더 특화된 특성을 인코딩 따라서 하위 층으로 갈수록 미세 조정에 대한 효과가 감소
5. 6-(3) 30
5. 6-(4)
5. 6-(5) 장기의존성 문제 - 입력과 출력 사이의 거리가 멀어질 수록 연관관계가 적어지는 문제

7. 8-(1) 질문-응답 모델은 2개의 입력을 가지고, 모델을 출력하는데 
가장 간단한 구조는 미리 정의한 어휘 사전에서 소프트 맥스 함수를 통해 한 단어로 된 답을 출력하는 것
7. 8-(2)    ① keras.callbacks.EarlyStopping - 과대적합이 일어나기 전에 일찍 훈련을 멈춤
	 ② keras.callbacks.LearningRateScheduler - 에포크에 따라 학습률을 조정
7. 8-(3) 탐욕적 샘플링 - 반복적이고 예상 가능한 문자열을 만든다.
          확률적 샘플링 - 소프트 맥스 결과값에서 가중치에 따라 랜덤으로 선택
          소프트맥스 역할 - 샘플링 과정에서 확률적인 양을 조절하기위해 사용
7. 8-(4) 옥타브
7. 8-(5) Z 포인트가 남자 얼굴의 임베딩된 표현이기 때문에 잠재공간의 Z + S 포인트는 남자 얼굴이 웃고있는 이미지가 될 것임
7. 8-(6) 생성자-생성자를 받아 실제 데이터와 비슷한 데이터를 만들어내도록 학습
          구분자- 실제 데이터와 생성자가 생성한 가짜 데이터를 구별하도록 학습
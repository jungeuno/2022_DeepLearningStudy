1,2-(1) O
1,2-(2) 표현학습
1,2-(3) 옵티마이저
1,2-(4) x의 행 벡터와 y의 열 벡터가 같은 크기여야 하므로 자동으로 x의 너비는 y의 높이와 동일해야 한다
(a, b, c, d) * (d) = (a, b, c)
(a, b, c, d) * (d, e) = (a, b, c, e)
1,2-(5) 모멘텀을 사용하면 가중치 값이 바로 바뀌지 않고 어느 정도 일정한 방향을 유지하면서 움직이게 되며, 가속도처럼 같은 방향으로 더 많이 변화시켜 학습속도를 높여줘 빠른 학습을 하게 한다.
3,4-(1) 모든 신경망은 단지 손실 함수를 최소화하기만 하며, 네트워크가 손실을 최소화 하기 위해 편법을 사용할 수 있기 때문이다.
3,4-(2) 3
3,4-(3) 5
3,4-(4) 알고리즘
3,4-(5) X
5,6-(1) 평행 이동 불변성이란 이미지의 어떤 패턴을 학습했다는 가정 하에, 다른 위치에서도 패턴 인식이 가능한 것을 뜻한다.
5,6-(2) 하위 층은 좀 더 일반적이고 재사용 가능한 특성들을 인코딩 하지만, 상위층은 좀 더 특화된 특성을 인코딩 하기 때문이다.
5,6-(3) ?
 텍스트 1 = 여유분은 제로패딩 
 텍스트 2 = 30이 넘는 범위부터 삭제
5,6-(4) 동일한 드롭아웃 마스크를 모든 타임스텝에 적용할 경우이다.
5,6-(5) 래디언트 소실 문제 = 학습 시간이 길어지면 학습 내용 소실 위험성이 커지는 것
7,8-(1) ?
7,8-(2) 
① keras.callbacks.EarlyStopping = 검증 손실이 향상되지 않을 경우 훈련을 중지시키는는 기능이다
② keras.callbacks.LearningRateScheduler = 에포크에 따라 학습률을 조정하는 기능이다.
7,8-(3) 
7,8-(4)옥타브
7,8-(5)남자가 웃고있는 이미지
7,8-(6)